{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3724153,"sourceType":"datasetVersion","datasetId":2151228}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==================== Imports ====================\nimport os\nimport itertools\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\n\n# ==================== Paths ====================\nbase_path = \"/kaggle/input/person-face-sketches\"\ntrain_real_dir = f\"{base_path}/train/photos\"\ntrain_sketch_dir = f\"{base_path}/train/sketches\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ==================== Dataset ====================\nclass FaceSketchDataset(Dataset):\n    def __init__(self, real_dir, sketch_dir, transform=None):\n        self.real_images = sorted(os.listdir(real_dir))\n        self.sketch_images = sorted(os.listdir(sketch_dir))\n        self.real_dir = real_dir\n        self.sketch_dir = sketch_dir\n        self.transform = transform\n\n    def __len__(self):\n        return min(len(self.real_images), len(self.sketch_images))\n\n    def __getitem__(self, idx):\n        real_img = Image.open(os.path.join(self.real_dir, self.real_images[idx])).convert(\"RGB\")\n        sketch_img = Image.open(os.path.join(self.sketch_dir, self.sketch_images[idx])).convert(\"RGB\")\n        \n        if self.transform:\n            real_img = self.transform(real_img)\n            sketch_img = self.transform(sketch_img)\n\n        return {\"real\": real_img, \"sketch\": sketch_img}\n\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),  # Reduced size from 256x256 to 128x128\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntrain_dataset = FaceSketchDataset(train_real_dir, train_sketch_dir, transform)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True)\n\n# ==================== Generator ====================\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        def downsample(in_feat, out_feat, norm=True):\n            layers = [nn.Conv2d(in_feat, out_feat, 4, 2, 1)]\n            if norm:\n                layers.append(nn.InstanceNorm2d(out_feat))\n            layers.append(nn.ReLU(inplace=True))\n            return layers\n\n        def upsample(in_feat, out_feat):\n            return [\n                nn.ConvTranspose2d(in_feat, out_feat, 4, 2, 1),\n                nn.InstanceNorm2d(out_feat),\n                nn.ReLU(inplace=True)\n            ]\n\n        self.model = nn.Sequential(\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(3, 64, 7),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n\n            *downsample(64, 128),\n            *downsample(128, 256),\n\n            *[ResidualBlock(256) for _ in range(3)],  # Reduced from 6 to 3\n\n            *upsample(256, 128),\n            *upsample(128, 64),\n\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(64, 3, 7),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# ==================== Discriminator ====================\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        def layer(in_c, out_c, norm=True):\n            layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n            if norm:\n                layers.append(nn.InstanceNorm2d(out_c))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *layer(3, 64, norm=False),\n            *layer(64, 128),\n            *layer(128, 256),\n            *layer(256, 512),\n            nn.Conv2d(512, 1, 4, 1, 1)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# ==================== Initialize Models ====================\nG_S2R = Generator().to(device)  # Sketch → Real\nG_R2S = Generator().to(device)  # Real → Sketch\nD_R = Discriminator().to(device)\nD_S = Discriminator().to(device)\n\n# ==================== Losses ====================\ncriterion_GAN = nn.MSELoss()\ncriterion_cycle = nn.L1Loss()\ncriterion_identity = nn.L1Loss()\n\n# ==================== Optimizers ====================\nlr = 0.0002\nbeta1 = 0.5\n\noptimizer_G = optim.Adam(itertools.chain(G_S2R.parameters(), G_R2S.parameters()), lr=lr, betas=(beta1, 0.999))\noptimizer_D_R = optim.Adam(D_R.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizer_D_S = optim.Adam(D_S.parameters(), lr=lr, betas=(beta1, 0.999))\n\n# ==================== Training ====================\nEPOCHS = 3\nsave_dir = \"/kaggle/working/saved_models\"\nos.makedirs(save_dir, exist_ok=True)\n\nfor epoch in range(1, EPOCHS + 1):\n    for i, batch in enumerate(train_loader):\n        real = batch[\"real\"].to(device)\n        sketch = batch[\"sketch\"].to(device)\n\n        # === Train Generators ===\n        optimizer_G.zero_grad()\n\n        fake_real = G_S2R(sketch)\n        pred_fake_real = D_R(fake_real)\n        valid = torch.ones_like(pred_fake_real, device=device)\n        fake = torch.zeros_like(pred_fake_real, device=device)\n        loss_GAN_S2R = criterion_GAN(pred_fake_real, valid)\n\n        fake_sketch = G_R2S(real)\n        pred_fake_sketch = D_S(fake_sketch)\n        valid = torch.ones_like(pred_fake_sketch, device=device)\n        loss_GAN_R2S = criterion_GAN(pred_fake_sketch, valid)\n\n        recovered_real = G_S2R(fake_sketch)\n        loss_cycle_real = criterion_cycle(recovered_real, real)\n\n        recovered_sketch = G_R2S(fake_real)\n        loss_cycle_sketch = criterion_cycle(recovered_sketch, sketch)\n\n        loss_identity_real = criterion_identity(G_S2R(real), real)\n        loss_identity_sketch = criterion_identity(G_R2S(sketch), sketch)\n\n        loss_G = (\n            loss_GAN_S2R + loss_GAN_R2S\n            + 10.0 * (loss_cycle_real + loss_cycle_sketch)\n            + 5.0 * (loss_identity_real + loss_identity_sketch)\n        )\n        loss_G.backward()\n        optimizer_G.step()\n\n        # === Train Discriminator R ===\n        optimizer_D_R.zero_grad()\n        pred_real = D_R(real)\n        valid = torch.ones_like(pred_real, device=device)\n        fake = torch.zeros_like(pred_real, device=device)\n        loss_D_real = criterion_GAN(pred_real, valid)\n\n        pred_fake = D_R(fake_real.detach())\n        loss_D_fake = criterion_GAN(pred_fake, fake)\n\n        loss_D_R_total = 0.5 * (loss_D_real + loss_D_fake)\n        loss_D_R_total.backward()\n        optimizer_D_R.step()\n\n        # === Train Discriminator S ===\n        optimizer_D_S.zero_grad()\n        pred_real = D_S(sketch)\n        valid = torch.ones_like(pred_real, device=device)\n        fake = torch.zeros_like(pred_real, device=device)\n        loss_D_real = criterion_GAN(pred_real, valid)\n\n        pred_fake = D_S(fake_sketch.detach())\n        loss_D_fake = criterion_GAN(pred_fake, fake)\n\n        loss_D_S_total = 0.5 * (loss_D_real + loss_D_fake)\n        loss_D_S_total.backward()\n        optimizer_D_S.step()\n\n        if i % 100 == 0:\n            print(f\"[Epoch {epoch}/{EPOCHS}] [Batch {i}/{len(train_loader)}] \"\n                  f\"[G loss: {loss_G.item():.4f}] \"\n                  f\"[D_R loss: {loss_D_R_total.item():.4f}] \"\n                  f\"[D_S loss: {loss_D_S_total.item():.4f}]\")\n\n    # Save generated samples\n    save_image(fake_real * 0.5 + 0.5, f\"/kaggle/working/fake_real_epoch_{epoch}.png\")\n    save_image(fake_sketch * 0.5 + 0.5, f\"/kaggle/working/fake_sketch_epoch_{epoch}.png\")\n\n    # Save models\n    torch.save(G_S2R.state_dict(), f\"{save_dir}/G_S2R_epoch_{epoch}.pth\")\n    torch.save(G_R2S.state_dict(), f\"{save_dir}/G_R2S_epoch_{epoch}.pth\")\n    torch.save(D_R.state_dict(), f\"{save_dir}/D_R_epoch_{epoch}.pth\")\n    torch.save(D_S.state_dict(), f\"{save_dir}/D_S_epoch_{epoch}.pth\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:27:26.101425Z","iopub.execute_input":"2025-04-12T13:27:26.101646Z","iopub.status.idle":"2025-04-12T14:54:09.161688Z","shell.execute_reply.started":"2025-04-12T13:27:26.101628Z","shell.execute_reply":"2025-04-12T14:54:09.161090Z"}},"outputs":[{"name":"stdout","text":"[Epoch 1/3] [Batch 0/2582] [G loss: 22.3031] [D_R loss: 0.6487] [D_S loss: 0.7607]\n[Epoch 1/3] [Batch 100/2582] [G loss: 5.5232] [D_R loss: 0.2255] [D_S loss: 0.1530]\n[Epoch 1/3] [Batch 200/2582] [G loss: 4.9291] [D_R loss: 0.3141] [D_S loss: 0.1451]\n[Epoch 1/3] [Batch 300/2582] [G loss: 5.0709] [D_R loss: 0.2061] [D_S loss: 0.1806]\n[Epoch 1/3] [Batch 400/2582] [G loss: 3.9239] [D_R loss: 0.2555] [D_S loss: 0.1323]\n[Epoch 1/3] [Batch 500/2582] [G loss: 4.4528] [D_R loss: 0.2359] [D_S loss: 0.1401]\n[Epoch 1/3] [Batch 600/2582] [G loss: 3.9457] [D_R loss: 0.1988] [D_S loss: 0.1777]\n[Epoch 1/3] [Batch 700/2582] [G loss: 4.0915] [D_R loss: 0.2212] [D_S loss: 0.1403]\n[Epoch 1/3] [Batch 800/2582] [G loss: 4.1181] [D_R loss: 0.1736] [D_S loss: 0.0601]\n[Epoch 1/3] [Batch 900/2582] [G loss: 3.6196] [D_R loss: 0.2351] [D_S loss: 0.1396]\n[Epoch 1/3] [Batch 1000/2582] [G loss: 3.7641] [D_R loss: 0.2116] [D_S loss: 0.1935]\n[Epoch 1/3] [Batch 1100/2582] [G loss: 3.9031] [D_R loss: 0.2519] [D_S loss: 0.1693]\n[Epoch 1/3] [Batch 1200/2582] [G loss: 3.0432] [D_R loss: 0.1894] [D_S loss: 0.2019]\n[Epoch 1/3] [Batch 1300/2582] [G loss: 3.1704] [D_R loss: 0.2152] [D_S loss: 0.1347]\n[Epoch 1/3] [Batch 1400/2582] [G loss: 5.0432] [D_R loss: 0.1913] [D_S loss: 0.0844]\n[Epoch 1/3] [Batch 1500/2582] [G loss: 3.6261] [D_R loss: 0.1697] [D_S loss: 0.1800]\n[Epoch 1/3] [Batch 1600/2582] [G loss: 4.7169] [D_R loss: 0.1304] [D_S loss: 0.2313]\n[Epoch 1/3] [Batch 1700/2582] [G loss: 3.8522] [D_R loss: 0.1395] [D_S loss: 0.1004]\n[Epoch 1/3] [Batch 1800/2582] [G loss: 3.3456] [D_R loss: 0.1482] [D_S loss: 0.1156]\n[Epoch 1/3] [Batch 1900/2582] [G loss: 4.8448] [D_R loss: 0.1762] [D_S loss: 0.0565]\n[Epoch 1/3] [Batch 2000/2582] [G loss: 4.0788] [D_R loss: 0.1847] [D_S loss: 0.0452]\n[Epoch 1/3] [Batch 2100/2582] [G loss: 3.2642] [D_R loss: 0.1480] [D_S loss: 0.1144]\n[Epoch 1/3] [Batch 2200/2582] [G loss: 3.4247] [D_R loss: 0.1433] [D_S loss: 0.1447]\n[Epoch 1/3] [Batch 2300/2582] [G loss: 3.6330] [D_R loss: 0.1828] [D_S loss: 0.1734]\n[Epoch 1/3] [Batch 2400/2582] [G loss: 3.8725] [D_R loss: 0.1711] [D_S loss: 0.1269]\n[Epoch 1/3] [Batch 2500/2582] [G loss: 4.4171] [D_R loss: 0.1624] [D_S loss: 0.0174]\n[Epoch 2/3] [Batch 0/2582] [G loss: 4.1219] [D_R loss: 0.1230] [D_S loss: 0.0452]\n[Epoch 2/3] [Batch 100/2582] [G loss: 3.3634] [D_R loss: 0.1925] [D_S loss: 0.2119]\n[Epoch 2/3] [Batch 200/2582] [G loss: 3.7352] [D_R loss: 0.0996] [D_S loss: 0.0458]\n[Epoch 2/3] [Batch 300/2582] [G loss: 5.0796] [D_R loss: 0.1693] [D_S loss: 0.1069]\n[Epoch 2/3] [Batch 400/2582] [G loss: 3.6775] [D_R loss: 0.1479] [D_S loss: 0.0278]\n[Epoch 2/3] [Batch 500/2582] [G loss: 3.0707] [D_R loss: 0.1421] [D_S loss: 0.1734]\n[Epoch 2/3] [Batch 600/2582] [G loss: 4.3214] [D_R loss: 0.1728] [D_S loss: 0.1237]\n[Epoch 2/3] [Batch 700/2582] [G loss: 3.6668] [D_R loss: 0.2244] [D_S loss: 0.0441]\n[Epoch 2/3] [Batch 800/2582] [G loss: 3.1167] [D_R loss: 0.1258] [D_S loss: 0.0857]\n[Epoch 2/3] [Batch 900/2582] [G loss: 3.5727] [D_R loss: 0.1065] [D_S loss: 0.2887]\n[Epoch 2/3] [Batch 1000/2582] [G loss: 3.3157] [D_R loss: 0.1613] [D_S loss: 0.0273]\n[Epoch 2/3] [Batch 1100/2582] [G loss: 3.2506] [D_R loss: 0.1881] [D_S loss: 0.1155]\n[Epoch 2/3] [Batch 1200/2582] [G loss: 3.2197] [D_R loss: 0.1384] [D_S loss: 0.0861]\n[Epoch 2/3] [Batch 1300/2582] [G loss: 3.4113] [D_R loss: 0.1653] [D_S loss: 0.0161]\n[Epoch 2/3] [Batch 1400/2582] [G loss: 4.0335] [D_R loss: 0.1306] [D_S loss: 0.1127]\n[Epoch 2/3] [Batch 1500/2582] [G loss: 3.1008] [D_R loss: 0.1138] [D_S loss: 0.0872]\n[Epoch 2/3] [Batch 1600/2582] [G loss: 4.3618] [D_R loss: 0.1622] [D_S loss: 0.3911]\n[Epoch 2/3] [Batch 1700/2582] [G loss: 3.2562] [D_R loss: 0.2286] [D_S loss: 0.0396]\n[Epoch 2/3] [Batch 1800/2582] [G loss: 3.6295] [D_R loss: 0.0907] [D_S loss: 0.1363]\n[Epoch 2/3] [Batch 1900/2582] [G loss: 3.7210] [D_R loss: 0.1597] [D_S loss: 0.0530]\n[Epoch 2/3] [Batch 2000/2582] [G loss: 3.3935] [D_R loss: 0.1174] [D_S loss: 0.0202]\n[Epoch 2/3] [Batch 2100/2582] [G loss: 2.6266] [D_R loss: 0.1588] [D_S loss: 0.1425]\n[Epoch 2/3] [Batch 2200/2582] [G loss: 4.6433] [D_R loss: 0.1704] [D_S loss: 0.0178]\n[Epoch 2/3] [Batch 2300/2582] [G loss: 3.6746] [D_R loss: 0.1582] [D_S loss: 0.0422]\n[Epoch 2/3] [Batch 2400/2582] [G loss: 3.6598] [D_R loss: 0.1485] [D_S loss: 0.0130]\n[Epoch 2/3] [Batch 2500/2582] [G loss: 3.8957] [D_R loss: 0.1726] [D_S loss: 0.0135]\n[Epoch 3/3] [Batch 0/2582] [G loss: 3.7105] [D_R loss: 0.1335] [D_S loss: 0.0254]\n[Epoch 3/3] [Batch 100/2582] [G loss: 2.4403] [D_R loss: 0.1736] [D_S loss: 0.1975]\n[Epoch 3/3] [Batch 200/2582] [G loss: 4.7716] [D_R loss: 0.1493] [D_S loss: 0.0237]\n[Epoch 3/3] [Batch 300/2582] [G loss: 2.9025] [D_R loss: 0.1648] [D_S loss: 0.1329]\n[Epoch 3/3] [Batch 400/2582] [G loss: 3.4915] [D_R loss: 0.1401] [D_S loss: 0.0168]\n[Epoch 3/3] [Batch 500/2582] [G loss: 3.6243] [D_R loss: 0.1340] [D_S loss: 0.0768]\n[Epoch 3/3] [Batch 600/2582] [G loss: 3.3660] [D_R loss: 0.1840] [D_S loss: 0.0648]\n[Epoch 3/3] [Batch 700/2582] [G loss: 3.0091] [D_R loss: 0.1675] [D_S loss: 0.0900]\n[Epoch 3/3] [Batch 800/2582] [G loss: 3.8710] [D_R loss: 0.1678] [D_S loss: 0.0607]\n[Epoch 3/3] [Batch 900/2582] [G loss: 2.8563] [D_R loss: 0.1700] [D_S loss: 0.1084]\n[Epoch 3/3] [Batch 1000/2582] [G loss: 3.5418] [D_R loss: 0.1311] [D_S loss: 0.0398]\n[Epoch 3/3] [Batch 1100/2582] [G loss: 2.7741] [D_R loss: 0.1786] [D_S loss: 0.0693]\n[Epoch 3/3] [Batch 1200/2582] [G loss: 3.8269] [D_R loss: 0.1338] [D_S loss: 0.0562]\n[Epoch 3/3] [Batch 1300/2582] [G loss: 3.2645] [D_R loss: 0.1352] [D_S loss: 0.1180]\n[Epoch 3/3] [Batch 1400/2582] [G loss: 2.9482] [D_R loss: 0.1609] [D_S loss: 0.1062]\n[Epoch 3/3] [Batch 1500/2582] [G loss: 2.9595] [D_R loss: 0.1226] [D_S loss: 0.1374]\n[Epoch 3/3] [Batch 1600/2582] [G loss: 3.5744] [D_R loss: 0.0970] [D_S loss: 0.0192]\n[Epoch 3/3] [Batch 1700/2582] [G loss: 3.4041] [D_R loss: 0.1776] [D_S loss: 0.0060]\n[Epoch 3/3] [Batch 1800/2582] [G loss: 3.1242] [D_R loss: 0.1390] [D_S loss: 0.0643]\n[Epoch 3/3] [Batch 1900/2582] [G loss: 3.4629] [D_R loss: 0.1528] [D_S loss: 0.0202]\n[Epoch 3/3] [Batch 2000/2582] [G loss: 3.1614] [D_R loss: 0.1576] [D_S loss: 0.0597]\n[Epoch 3/3] [Batch 2100/2582] [G loss: 3.0284] [D_R loss: 0.1884] [D_S loss: 0.0466]\n[Epoch 3/3] [Batch 2200/2582] [G loss: 3.3305] [D_R loss: 0.1682] [D_S loss: 0.0072]\n[Epoch 3/3] [Batch 2300/2582] [G loss: 3.8080] [D_R loss: 0.1048] [D_S loss: 0.0313]\n[Epoch 3/3] [Batch 2400/2582] [G loss: 3.5764] [D_R loss: 0.1250] [D_S loss: 0.0056]\n[Epoch 3/3] [Batch 2500/2582] [G loss: 3.5586] [D_R loss: 0.1512] [D_S loss: 0.1009]\n","output_type":"stream"}],"execution_count":1}]}
